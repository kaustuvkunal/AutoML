<text>
## 5. Data Modelling

Lets create Data models.
First we will define few useful functions

</text>



<text>
##### Function to evalute model
</text>


<code>

metric = "<_METRIC>"
def model_evaluate(model, metric=metric ):
    mesure = cross_val_score(model,train_df, y, cv=3, scoring=metric )
    mesure_mean = mesure.mean()
    print("3 Fold Cross-Validation " +metric+" measure is : " +str(mesure_mean))
    return  mesure_mean
    
</code>

<text>
#####  Function to plot auc curve 
</text>
<code>
def generate_auc(y_valid, valp, model_name):
    auc_scr = roc_auc_score(y_valid, valp)
    print('The AUC for ' +model_name+ ' is :', auc_scr)

    fpr, tpr, thresholds = roc_curve(y_valid, valp)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6,5))
    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'purple', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'upper left')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])

    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()
    
</code>

<text>
##### Function to plot Confusion Matrix 
</text>

<code>
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(6,5));
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    plt.grid(False)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", 
                 color="white" if cm[i, j] > thresh else "black")
        
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

</code>

<text>
### 5.1 Base Models
</text>

<text>
#### 5.1.1 Logistic Regression

##### Training  
</text>


<code>

model1 = LogisticRegression()
model1.fit(X_train,y_train)
y_pred = model1.predict(X_valid)
model_evaluate(model1)

# this is for binary classification  only 
if len(distinct_Y) == 2:
    generate_auc(y_valid, y_pred, model_name="logistic regression")
</code>


<text>
##### Ploting Confusion Matrix
</text>

<code>

cnf_matrix = confusion_matrix(y_valid, y_pred)
np.set_printoptions(precision=2)
plt.figure(figsize=(8,8))
plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')
plt.show()
</code>

<text>
##### Hyperparameter tuning using RandomSearchCV
</text>

<code>

params = {
        "C" : [  1, 0.1, 0.05   ],
        "max_iter" : [ 100, 500, 1000],
        "multi_class" : ['auto'],
           }

model1_rs = RandomizedSearchCV( model1, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model1 = model1_rs.fit(X_train, y_train)
model1_measure =  model_evaluate(best_model1)

</code>

<text>
##### Model definition after hyperparameter tuning
</text>


<code>
model1_def = best_model1.best_estimator_
best_model1.best_estimator_
</code>



<text>
#### 5.1.1 DecisionTreeClassifier

##### Training  

</text>



<code>
model2 = DecisionTreeClassifier()
model2.fit(X_train,y_train)
y_pred = model2.predict(X_valid)
model_evaluate(model2)

# this is for binary classification  only 
if len(distinct_Y) == 2:
    generate_auc(y_valid, y_pred, model_name="DecisionTreeClassifier")
    
</code>


<text>
##### Ploting Confusion Matrix
</text>


<code>

cnf_matrix = confusion_matrix(y_valid, y_pred)
np.set_printoptions(precision=2)
plt.figure(figsize=(8,8))
plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')
plt.show()
</code>

<text>
##### Hyperparameter tuning using RandomSearchCV
</text>

<code>

params = {
        "max_depth": [  4, 10, 16, 20   ],
        "criterion": [ 'gini', 'entropy'],
           }

model2_rs = RandomizedSearchCV( model2, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model2 = model2_rs.fit(X_train, y_train)
model2_measure =  model_evaluate(best_model2)

</code>

<text>
##### Model definition after hyperparameter tuning
</text>

<code>
model2_def = best_model2.best_estimator_
best_model2.best_estimator_
</code>

 

<text>
#### 5.1.3 KNeighborsClassifier

##### Training  

</text>



<code>

model3 = KNeighborsClassifier()
model3.fit(X_train,y_train)
y_pred = model3.predict(X_valid)
model_evaluate(model3)

# this is for binary classification  only 
if len(distinct_Y) == 2:
    generate_auc(y_valid, y_pred, model_name="KNeighborsClassifier")
    
</code>


<text>

##### Ploting Confusion Matrix
</text>

<code>

cnf_matrix = confusion_matrix(y_valid, y_pred)
np.set_printoptions(precision=2)
plt.figure(figsize=(8,8))
plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')
plt.show()

</code>

<text>
##### Hyperparameter tuning using RandomSearchCV
</text>

<code>

params = {
        "n_neighbors": [  3, 5 ],         
        "algorithm" : [ 'auto' ],  
         }

model3_rs = RandomizedSearchCV( model3, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model3 = model3_rs.fit(X_train, y_train)
model3_measure =  model_evaluate(best_model3)

</code>

<text>
##### Model definition after hyperparameter tuning
</text>

<code>
model3_def = best_model3.best_estimator_
best_model3.best_estimator_
</code>


<text>
### 5.2 Bagging Ensemble Models

</text>


<text>
#### 5.2.1 RandomForestClassifier

##### Training  

</text>


<code>

model4 = RandomForestClassifier()
model4.fit(X_train,y_train)
y_pred = model4.predict(X_valid)
model_evaluate(model4)

# this is for binary classification  only 
if len(distinct_Y) == 2:
    generate_auc(y_valid, y_pred, model_name="RandomForestClassifier")
    
</code>


<text>

##### Ploting Confusion Matrix
</text>

<code>

cnf_matrix = confusion_matrix(y_valid, y_pred)
np.set_printoptions(precision=2)
plt.figure(figsize=(8,8))
plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')
plt.show()
</code>

<text>
##### Hyperparameter tuning using RandomSearchCV
</text>

<code>

params = {
        "n_estimators": [ 100, 500 ],
        "criterion" : ["entropy", "gini"],
        "max_depth" : [ 10, 30],
        "max_features" : [ .30,  .70 ],    
        "bootstrap" : [True, False]
           }

model4_rs = RandomizedSearchCV( model4, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model4 = model4_rs.fit(X_train, y_train)
model4_measure =  model_evaluate(best_model4)

</code>

<text>
##### Model definition after hyperparameter tuning
</text>

<code>
model4_def = best_model4.best_estimator_
best_model4.best_estimator_
</code>


<text>
#### 5.2.2 ExtraTreesClassifier

##### Training  

</text>


<code>

model5 = ExtraTreesClassifier()
model5.fit(X_train,y_train)
y_pred = model5.predict(X_valid)
model_evaluate(model5)

# this is for binary classification  only 
if len(distinct_Y) == 2:
    generate_auc(y_valid, y_pred, model_name="ExtraTreesClassifier")
    
</code>


<text>

##### Ploting Confusion Matrix
</text>

<code>

cnf_matrix = confusion_matrix(y_valid, y_pred)
np.set_printoptions(precision=2)
plt.figure(figsize=(8,8))
plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')
plt.show()
</code>

<text>
##### Hyperparameter tuning using RandomSearchCV
</text>

<code>

params = {
        "n_estimators": [ 200, 600, 1500 ],         
        "max_depth" : [ 10, 30 ],
        "max_features" : ['auto', None ],
           }

model5_rs = RandomizedSearchCV( model5, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model5 = model5_rs.fit(X_train, y_train)
model5_measure =  model_evaluate(best_model5)

</code>

<text>
##### Model definition after hyperparameter tuning
</text>

<code>
model5_def = best_model5.best_estimator_
best_model5.best_estimator_
</code>


<text>
### 5.3 Boosting Ensemble Models

</text>


<text>
#### 5.3.1 XGBClassifier

##### Training  

</text>

<code>

model6 = XGBClassifier()
model6.fit(X_train,y_train)
y_pred = model6.predict(X_valid)
model_evaluate(model6)

# this is for binary classification  only 
if len(distinct_Y) == 2:
    generate_auc(y_valid, y_pred, model_name="XGBClassifier")
    
</code>


<text>

##### Ploting Confusion Matrix
</text>

<code>

cnf_matrix = confusion_matrix(y_valid, y_pred)
np.set_printoptions(precision=2)
plt.figure(figsize=(8,8))
plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')
plt.show()
</code>

<text>
##### Hyperparameter tuning using RandomSearchCV
</text>

<code>

params = {
        "learning_rate": [  0.1, 0.01 ],
        "n_estimators" : [ 400, 800 ],
        "max_depth" : [ 15, 30 ],       
           }

model6_rs = RandomizedSearchCV( model6, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model6 = model6_rs.fit(X_train, y_train)
model6_measure =  model_evaluate(best_model6)

</code>

<text>
##### Model definition after hyperparameter tuning
</text>

<code>
model6_def = best_model6.best_estimator_
best_model6.best_estimator_
</code>

<text>
#### 5.3.2 LGBMClassifier

##### Training  

</text>


<code>

model7 = LGBMClassifier()
model7.fit(X_train,y_train)
y_pred = model7.predict(X_valid)
model_evaluate(model7)

# this is for binary classification  only 
if len(distinct_Y) == 2:
    generate_auc(y_valid, y_pred, model_name="LGBMClassifier")
    
</code>


<text>

##### Ploting Confusion Matrix
</text>

<code>

cnf_matrix = confusion_matrix(y_valid, y_pred)
np.set_printoptions(precision=2)
plt.figure(figsize=(8,8))
plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')
plt.show()
</code>

<text>
##### Hyperparameter tuning using RandomSearchCV
</text>

<code>

params = {
        "learning_rate": [  0.01, 0.1  ],
        "num_iterations" : [ 100, 600],
        "num_leaves" : [  10, 30 ],
           }

model7_rs = RandomizedSearchCV( model7, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model7 = model7_rs.fit(X_train, y_train)
model7_measure =  model_evaluate(best_model7)

</code>

<text>
##### Model definition after hyperparameter tuning
</text>

<code>
model7_def = best_model7.best_estimator_
best_model7.best_estimator_
</code>


<text>

### How fair our Models performed so far ?

</text>

<code>

data = [['Logistic Regression',model1_def, model1_measure],['Decision Tree',model2_def, model2_measure],
['KNN',model3_def, model3_measure],['Random Forest',model4_def, model4_measure],
['Extra Tree',model5_def, model5_measure],['XGBoost',model6_def, model6_measure],
['LightGBM',model7_def, model7_measure] ]
models = pd.DataFrame( data, columns = ['Model', 'Model_Definition', 'Score']) 


models_df = models.sort_values(by='Score', ascending=False)
models_df = models_df.set_index('Score')
models_df.head(7)

</code>


 
<text>
### 6.1 Voting Classifiers

For our voting classfier, we will take 3 models one base model,one bagging and one boosting model 

</text>

<code>
#Base model
m1 = models.loc[models['Score'] == max(model1_measure, model1_measure, model3_measure), 
              'Model_Definition'].iloc[0]
m1
</code>


<code>
#Bagging model
m2 = models.loc[models['Score'] == max(model4_measure, model5_measure), 
              'Model_Definition'].iloc[0]
m2
</code>

<code>
# Boosting model
m3 = models.loc[models['Score'] == max(model6_measure,  model7_measure), 
              'Model_Definition'].iloc[0]
m3
</code>


<text>

#### 6.1.1 Hard Voting Classifier

</text>

<code>

model8 = VotingClassifier(
    estimators=[('base', m1), ('bagging', m2),('boosting',m3)],
    voting = 'hard' )

model8.fit(X_train,y_train)
y_pred = model8.predict(X_valid)
model8_measure = model_evaluate(model8)

# adding into our model list 
models = models.append([{ 'Model': "Hard Voting Classifier", 'Model_Definition': model8, 'Score': model8_measure  }])

</code>




<text>
#### 6.1.2 Soft Voting Classifier
</text>

<code>
model9  = VotingClassifier(
    estimators=[('base', m1), ('bagging', m2),('boosting',m3)],
    voting = 'soft' )
model9.fit(X_train,y_train)
y_pred = model9.predict(X_valid)
model9_measure = model_evaluate(model9)
 

# add into our model list 
models = models.append([{ 'Model': "Hard Voting Classifier", 'Model_Definition': model9, 'Score': model9_measure  }])

</code>
 
 
 <text>
 ### 6.2 Stacking Model
 </text>

<code>

from mlxtend.classifier import StackingCVClassifier
lr = LogisticRegression() 
model10 = StackingCVClassifier(classifiers=[m1,  m2,  m3],
                            use_probas=True,
                            meta_classifier=lr)

 
X_train_matrix = X_train.as_matrix()
y_train_matrix = y_train.as_matrix()

model10.fit(X_train_matrix,y_train_matrix)
train_df_matrix = train_df.as_matrix()
mesure = cross_val_score(model10,train_df_matrix, y, cv=3, scoring= metric )
model10_measure = mesure.mean()
print("3 Fold Cross-Validation measure is : " +str(model10_measure))

models = models.append([{ 'Model': "Stacking Model", 'Model_Definition': model10, 'Score': model10_measure  }])

</code>

<text>
#### Listing Models Scores  
</text>


<code>

models_df = models.sort_values(by='Score', ascending=False)
models_df = models_df.set_index('Score')
models_df.head(10)

</code>

<text>
#### Best model 
</text>


<code>

bestmodel = models_df.iloc[0]['Model_Definition']
bestmodel
</code>


<text>
## 7. Submission File

Preparing submission file for the best model

</text>

<code>
pred = bestmodel.predict(test_df)
sub = pd.DataFrame()
sub[_id] = test_id
sub[target] = pred
sub.to_csv("baseline_submission.csv", index=False)
print ("Submission File Generated, here is the snapshot: ")
print (sub.head(10))

</code>

