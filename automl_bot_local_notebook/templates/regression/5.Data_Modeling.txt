<text>
## 5. Data Modelling

Lets create Data models.
First we will define few useful functions

</text>



<text>
##### Function to evalute model
</text>


<code>

metric = "<_METRIC>"
def model_evaluate(model, metric=metric ):
    measure = cross_val_score(model,train_df, y, cv=3, scoring=metric )
    measure_mean = measure.mean()
    print("3 Fold Cross-Validation " +metric+" measure is : " +str(measure_mean))
    return  measure_mean
    
</code>

 

<text>
### 5.1 Base Models
</text>

<code>

if (cat_col_flag & pca_flag  ):
    base_pipe = Pipeline([
                ('onehot',OneHotEncoder(handle_unknown='ignore')),
                ('pca',TruncatedSVD()),
                ('sscalar',  StandardScaler(with_mean=False)),
              ])
elif (cat_col_flag):
    base_pipe = Pipeline([
                ('onehot',OneHotEncoder(handle_unknown='ignore')),
                ('sscalar',  StandardScaler(with_mean=False)),
              ])
else:
    base_pipe = Pipeline([
                ('sscalar',  StandardScaler(with_mean=False)),
              ])


print('base model is :')
base_pipe
</code>


<code>

# Dataframe for model leaderboard
models = pd.DataFrame( columns = ['Model', 'Model_Definition', 'Score']) 

</code>

<text>
#### 5.1.1 Linear Ridge Regression

##### Training
</text>


<code>

model1 =  Ridge()

model1_pipe = Pipeline([
                 ('base' , base_pipe),
                ('Ridge', model1),
              ])

model1_pipe.fit(X_train, y_train)
y_pred = model1_pipe.predict(X_valid)
model1_pipe_measure = model_evaluate(model1_pipe)

</code>


<text>
##### Hyperparameter tuning using RandomSearchCV
</text>

<code>
params = {   
        "Ridge__alpha" : [ 1, 10, 100 ],
           }
model1_pipe_rs = RandomizedSearchCV( model1_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model1_pipe = model1_pipe_rs.fit(X_train, y_train)


model1_def = best_model1_pipe.best_estimator_

model1_measure =  model_evaluate(model1_def)

models = models.append({'Model': 'Ridge', 'Model_Definition': model1_def, 'Score': model1_measure},
                       ignore_index=True)

print('\n'+"Model parameters after parameter tuning")
best_model1_pipe.best_params_
</code>


<text>
#### 5.1.2 Linear Lasso Regression

##### Training

</text>

<code>
model2 =  Lasso()

model2_pipe = Pipeline([
                 ('base' , base_pipe),
                ('Lasso', model2),
              ])

model2_pipe.fit(X_train, y_train)
y_pred = model2_pipe.predict(X_valid)
model2_pipe_measure = model_evaluate(model2_pipe)
</code>


<text>
##### Hyperparameter tuning using RandomSearchCV
</text>


<code>
params = {
        "Lasso__alpha" : [ 0.001, 0.01, 0.1,1, 10, 100 ],
           }
model2_pipe_rs = RandomizedSearchCV( model2_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model2_pipe = model2_pipe_rs.fit(X_train, y_train)


model2_def = best_model2_pipe.best_estimator_

model2_measure =  model_evaluate(model2_def)

models = models.append({'Model': 'Lasso', 'Model_Definition': model2_def, 'Score': model2_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model2_pipe.best_params_
</code>


<text>
#### 5.1.3 ElasticNet

##### Training

</text>



<code>

model3 =  ElasticNet()

model3_pipe = Pipeline([
                 ('base' , base_pipe),
                ('ElasticNet', model3),
              ])

model3_pipe.fit(X_train, y_train)
y_pred = model3_pipe.predict(X_valid)
model3_pipe_measure = model_evaluate(model3_pipe)

</code>


<text>
##### Hyperparameter tuning using RandomSearchCV

</text>



<code>
params = {
        "ElasticNet__alpha" : [ 0.001, 0.01, 0.1,1, 10, 100 ],
           }
model3_pipe_rs = RandomizedSearchCV( model3_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model3_pipe = model3_pipe_rs.fit(X_train, y_train)

model3_def = best_model3_pipe.best_estimator_

model3_measure =  model_evaluate(model3_def)

models = models.append({'Model': 'ElasticNet', 'Model_Definition': model3_def, 'Score': model3_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model3_pipe.best_params_

</code>


<text>
#### 5.1.4  SVM Regressor
##### Training
</text>



<code>
model4 = SVR()
model4_pipe = Pipeline([
                 ('base' , base_pipe),
                ('SVR', model4),
              ])

model4_pipe.fit(X_train, y_train)
y_pred = model4_pipe.predict(X_valid)
model4_measure = model_evaluate(model4_pipe)

</code>


<text>
##### Hyperparameter tuning using RandomSearchCV

</text>



<code>
params = {
        "SVR__C": [  0.001, 0.01, 0.1, 1, 10 ],
        "SVR__gamma" : [ 0.001, 0.01, 0.1, 1],

           }

model4_pipe_rs = RandomizedSearchCV( model4_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model4_pipe = model4_pipe_rs.fit(X_train, y_train)


model4_def = best_model4_pipe.best_estimator_
model4_measure =  model_evaluate(model4_def)

models = models.append({'Model': 'SVR', 'Model_Definition': model4_def, 'Score': model4_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model4_pipe.best_params_
</code>





<text>
#### 5.1.5  KNeighborsRegressor

##### Training
</text>



<code>
from sklearn.neighbors import KNeighborsRegressor

model5 = KNeighborsRegressor()
model5_pipe = Pipeline([
                 ('base' , base_pipe),
                ('KNeighborsRegressor', model5),
              ])

model5_pipe.fit(X_train, y_train)
y_pred = model5_pipe.predict(X_valid)
model5_measure = model_evaluate(model5_pipe)

</code>


<text>
##### Hyperparameter tuning using RandomSearchCV

</text>



<code>
params = {
        "KNeighborsRegressor__n_neighbors": [  3, 5, 10 ],
        "KNeighborsRegressor__algorithm" : [ 'auto' ],
         }

model5_pipe_rs = RandomizedSearchCV( model5_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model5_pipe = model5_pipe_rs.fit(X_train, y_train)


model5_def = best_model5_pipe.best_estimator_

model5_measure =  model_evaluate(model5_def)

models = models.append({'Model': 'KNeighborsRegressor', 'Model_Definition': model5_def, 'Score': model5_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model5_pipe.best_params_
</code>









<text>
#### 5.1.6  Base Models leaderboard
We will take two top performing base model for our voting and stacking  model
</text>

<code>
base_models_df = models.sort_values(by='Score', ascending=False)
base_models_df = base_models_df.set_index('Score')
# selecting top two base model
base_model1 = base_models_df.iloc[0]['Model_Definition']
base_model2 = base_models_df.iloc[1]['Model_Definition']

base_models_df.head(5)
</code>





<text>
### 5.2 Bagging Models

</text>


<text>
#### 5.2.1 RandomForestRegressor

##### Training

</text>


<code>

model6 = RandomForestRegressor()
model6_pipe = Pipeline([
                 ('base' , base_pipe),
                ('RandomForestRegressor', model6),
              ])

model6_pipe.fit(X_train, y_train)
y_pred = model6_pipe.predict(X_valid)
mmodel6_pipe_measure = model_evaluate(model6_pipe)


</code>


<text>
##### Hyperparameter tuning using RandomSearchCV

</text>



<code>
params = {
        'RandomForestRegressor__n_estimators': range(50,450,150),


           }
model6_pipe_rs = RandomizedSearchCV( model6_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model6_pipe = model6_pipe_rs.fit(X_train, y_train)


model6_def = best_model6_pipe.best_estimator_

model6_measure =  model_evaluate(model6_def)

models = models.append({'Model': 'RandomForestRegressor', 'Model_Definition': model6_def, 'Score': model6_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model6_pipe.best_params_

</code>




<text>
#### 5.2.2 ExtraTreesRegressor

##### Training

</text>


<code>

model7 = ExtraTreesRegressor()
model7_pipe = Pipeline([
                 ('base' , base_pipe),
                ('ExtraTreesRegressor', model7),
              ])

model7_pipe.fit(X_train, y_train)
y_pred = model7_pipe.predict(X_valid)
model7_measure = model_evaluate(model7_pipe)

</code>


<text>
##### Hyperparameter tuning using RandomSearchCV

</text>



<code>
params = {
        'ExtraTreesRegressor__n_estimators': range(50,126,25),
        'ExtraTreesRegressor__max_features': ['auto', None ],
        'ExtraTreesRegressor__min_samples_leaf': range(20,50,10),
        'ExtraTreesRegressor__min_samples_split': range(15,30,15),

           }

model7_pipe_rs = RandomizedSearchCV( model7_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model7_pipe = model7_pipe_rs.fit(X_train, y_train)


model7_def = best_model7_pipe.best_estimator_

model7_measure =  model_evaluate(model7_def)

models = models.append({'Model': 'ExtraTreesRegressor', 'Model_Definition': model7_def, 'Score': model7_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model7_pipe.best_params_

</code>

<text>

#### Select top bagging model for our stacking & voting regressor
</text>

<code>
if (model6_measure > model7_measure):
    bagging_model = model6_def
else :
    bagging_model = model7_def

</code>


<text>
### 5.3 Boosting Ensemble Models

</text>


<text>
#### 5.3.1 XGBRegressor

##### Training

</text>

<code>

model8 = XGBRegressor( )
model8_pipe = Pipeline([
                 ('base' , base_pipe),
                ('XGBRegressor', model8),
              ])

model8_pipe.fit(X_train, y_train)
y_pred = model8_pipe.predict(X_valid)
model8_measure = model_evaluate(model8_pipe)


</code>


<text>
##### Hyperparameter tuning using RandomSearchCV

</text>



<code>
params = {
        "XGBRegressor__learning_rate": [  0.1, 0.01 ],
        "XGBRegressor__n_estimators" : [ 10, 100, 600],
        "XGBRegressor__objective" : [ 'reg:squarederror','reg:gamma'],
          "XGBRegressor__max_depth" : [ 4, 10 , 15 ],

           }

model8_pipe_rs = RandomizedSearchCV( model8_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)

best_model8_pipe = model8_pipe_rs.fit(X_train, y_train)
model8_def = best_model8_pipe.best_estimator_

model8_measure =  model_evaluate(model8_def)
models = models.append({'Model': 'XGBRegressor', 'Model_Definition': model8_def, 'Score': model8_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model8_pipe.best_params_

</code>



<text>
#### 5.3.2 LGBMRegressor

##### Training

</text>


<code>

model9 = LGBMRegressor( )
model9_pipe = Pipeline([
                 ('base' , base_pipe),
                ('LGBMRegressor', model9),
              ])

model9_pipe.fit(X_train, y_train)
y_pred = model9_pipe.predict(X_valid)
model9_measure = model_evaluate(model9_pipe)


</code>

<text>
##### Hyperparameter tuning using RandomSearchCV

</text>



<code>
params = {
        "LGBMRegressor__learning_rate": [  0.01, 0.1  ],
         "LGBMRegressor__num_iterations" : [ 100, 500 ],
           }

model9_pipe_rs = RandomizedSearchCV( model9_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)

best_model9_pipe = model9_pipe_rs.fit(X_train, y_train)
model9_def = best_model9_pipe.best_estimator_

model9_measure =  model_evaluate(model9_def)

models = models.append({'Model': 'LGBMRegressor', 'Model_Definition': model9_def, 'Score': model9_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model9_pipe.best_params_

</code>


<text>
#### 5.3.3 AdaBoostRegressor

##### Training

</text>


<code>

model10 =  AdaBoostRegressor()
model10_pipe = Pipeline([
                 ('base' , base_pipe),
                ('AdaBoostRegressor', model10),
              ])

model10_pipe.fit(X_train, y_train)
y_pred = model10_pipe.predict(X_valid)
model10_measure = model_evaluate(model10_pipe)

</code>

<text>
##### Hyperparameter tuning using RandomSearchCV

</text>



<code>
params = {
        "AdaBoostRegressor__learning_rate": [  0.01, 0.1, 1  ],
        "AdaBoostRegressor__n_estimators" : [ 100, 600, 1000],
           }

model10_pipe_rs = RandomizedSearchCV( model10_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)
best_model10_pipe = model10_pipe_rs.fit(X_train, y_train)


model10_def = best_model10_pipe.best_estimator_

model10_measure =  model_evaluate(model10_def)

models = models.append({'Model': 'AdaBoostRegressor', 'Model_Definition': model10_def, 'Score': model10_measure},
                       ignore_index=True)
print('\n'+"Model parameters after parameter tuning")
best_model10_pipe.best_params_
</code>

<text>

#### Selecting top bosting model for our stacking & voting regressor

</text>

<code>
if ( max(model10_measure,model9_measure,model8_measure) == model8_measure):
    boosting_model = model8_def
elif( max(model10_measure,model9_measure,model8_measure) == model9_measure):
    boosting_model = model9_def
else :
    boosting_model = model10_def
</code>


<text>

### 5.4 Model Leaderboard

</text>

<code>

models_df = models.sort_values(by='Score', ascending=False)
models_df = models_df.set_index('Score')
models_df.head(10)

</code>



<text>
## 6. Advance Modeling
For our Advance Modeling, we will take 4 models two from base and one each from bagging and boosting model
</text>



<text>
### 6.1 Voting Regressor (Averaging)

Averages the individual predictions to form a final prediction.
</text>



<code>

model11 = VotingRegressor([('base_model1',base_model1),('base_model2',base_model2), ('bagging_model', bagging_model),('boosting_model',boosting_model)] )
model11.fit(X_train,y_train)
y_pred = model11.predict(X_valid)
model11_measure = model_evaluate(model11)
# adding into our model list
models = models.append([{ 'Model': "Voting Regressor(Averaging)", 'Model_Definition': model11, 'Score': model11_measure }])

</code>



<text>

### 6.2 Voting Regressor (Weighted Averaging)

</text>

<code>

model12 = VotingRegressor([('base_model1',base_model1),('base_model2',base_model2), ('bagging_model', bagging_model),('boosting_model',boosting_model)], weights= [0.15,0.15,0.15,0.55] )
model12.fit(X_train,y_train)
y_pred = model12.predict(X_valid)
model12_measure = model_evaluate(model12)
# adding into our model list
models = models.append([{ 'Model': "Voting Regressor (Weighted Averaging)", 'Model_Definition': model12, 'Score': model12_measure }])
</code>



<text>

### 6.2 Stacking Model

</text>

<code>

lasso = Lasso()
model13 = StackingCVRegressor(regressors=(base_model1,bagging_model,boosting_model),
                            meta_regressor=lasso,
                            random_state= 2018)
X_train_matrix = X_train.as_matrix()
y_train_matrix = y_train.as_matrix()



model13.fit(X_train_matrix,y_train_matrix)
train_df_matrix = train_df.as_matrix()
mesure = cross_val_score(model13,train_df_matrix, y, cv=3, scoring= metric )

model13_measure = mesure.mean()
print("3 Fold Cross-Validation measure is : " +str(model13_measure))

models = models.append([{ 'Model': "StackingModel", 'Model_Definition': model13, 'Score': model13_measure }])

</code>


<text>
### 6.3 Final Model Leaderboard
</text>

<code>
models_df = models.sort_values(by='Score', ascending=False)
models_df = models_df.set_index('Score')
models_df.head(13)
</code>



<text>
## 7. Submission File
Preparing submission file for the best model
</text>


<code>

# function to perform unskewness
def consider_skewness(pred,sk_transform = skew_transform ):
    if sk_transform == 'log':
        return np.exp(pred)
    elif sk_transform == 'sqrt':
        return np.square(pred)
    elif sk_transform == 'square':
        return np.sqrt(pred)

    return pred
</code>

<text>
#### Generating Submission File
</text>


<code>

# best model from all model
tuned_best_model_def = models_df.iloc[0]['Model_Definition']
# prediction
pred = tuned_best_model_def.predict(test_df)
# unskewness
final_pred = consider_skewness(pred)

sub = pd.DataFrame()
sub[_id] = test_id
sub[target] = final_pred
sub.to_csv("baseline_solution.csv", index=False)
print ("Submission File Generated, here is the snapshot: ")
print (sub.head(10))
</code>

 