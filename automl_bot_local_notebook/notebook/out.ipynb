{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is auto generated by bot - : <B> Plexy </B><br/> \n",
    "An Auto-ML Baseline notebook solution for Classification and Regression problems \n",
    "\n",
    "<br/> <B>  author@ : Kaustuv Kunal</B>\n",
    "\n",
    "<br/>Learning type : Regression\n",
    " \n",
    "### Contents \n",
    "\n",
    "1. <B>Import Libraries</B> \n",
    "<br/> \n",
    "\n",
    "2.  <B>Read & Explore Data</B>  \n",
    "        2.1 Fetch Dataset    \n",
    "        2.2 Dataset Extract   \n",
    "        2.3 Dataset Summary   \n",
    "        2.4 Dataset Shape\n",
    "        2.5 Missing Features count \n",
    "        2.6 Segregate Features & Target  \n",
    "\n",
    "3. <B>Data Visualisation</B>\n",
    "        3.1 Target Distribution  \n",
    "        3.2 Correlation-Matrix (For Numeric Features) \n",
    "        3.3 Category Distribution (For Categorical Features)\n",
    "        3.4 Missing Values Distribution      \n",
    " \n",
    "4. <B>Data Prepration & Cleaning</B>\n",
    "\t\t4.1 Dropping Additional Features\n",
    "        4.1 Dropping Less Correlated Numeric Features\n",
    "        4.2 Missing Values Treatment     \n",
    "        4.3 Target Skewness Transform\n",
    "        4.4 One-hot-encoding and PCA Flag Setup      \n",
    "        4.5 Train Test Split \n",
    "\n",
    "5. <B>Data Modelling</B>    \n",
    "        5.1 Base Models     \n",
    "            5.1.1 Ridge Regression\n",
    "            5.1.2 Lesso Regression\n",
    "            5.1.3 ElasticNet   \n",
    "            5.1.4 SVM\n",
    "            5.1.5 KNeighbors    \n",
    "        5.2 Bagging Models      \n",
    "            5.2.1 Random Forest    \n",
    "            5.2.2 Extra Tree       \n",
    "        5.3 Boosting Models       \n",
    "            5.3.1 XGBoost     \n",
    "            5.3.2 LightGBM  \n",
    "            5.3.3 AdaBoost  \n",
    "        5.4 Model Leaderboard     \n",
    "<br/> \n",
    "6. <B>Advance Modeling</B>     \n",
    "        6.1 Voting Regressor (Average)\n",
    "        6.2 Voting Regressor (Weighted Average)      \n",
    "        6.3 Stacking Model \n",
    "        6.4 Final Model Leaderboard\n",
    "         \n",
    "7. <B>Submission File</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Import Libraries\n",
    "Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "## modelling libraries\n",
    "\n",
    "# read & explore Data\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import os \n",
    "\n",
    "# data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data  visualisation & prepration\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# data modeling\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print (\" .. libraries imported  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Read & Explore Data\n",
    "\n",
    "### 2.1 Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetch dataset\n",
    "\n",
    "ext = \"csv\"\n",
    "train_path = \"/Users/kaustuv/DataScience/DS_projects/AnalyticVidhya/Reg_BlackFridaySale/datasets/train.csv\"\n",
    "test_path = \"/Users/kaustuv/DataScience/DS_projects/AnalyticVidhya/Reg_BlackFridaySale/datasets/test.csv\"\n",
    "\n",
    "\n",
    "if( ext == \"csv\"):\n",
    "\ttrain_df = pd.read_csv(train_path)\n",
    "elif (ext == \"xlsx\"):\n",
    "\ttrain_df = pd.read_excel(train_path)\n",
    "\n",
    "train_copy = train_df.copy()\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "if os.path.exists(test_path):\n",
    "\tif( ext == \"csv\"):\n",
    "\t\ttest_df = pd.read_csv(test_path)\n",
    "\telif (ext == \"xlsx\"):\n",
    "\t\ttest_df = pd.read_excel(test_path)\n",
    "\t\n",
    "\n",
    "print (\" ... dataset fetched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset extract\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset  summary\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find categorical columns in the dataset \n",
    "num_cols = train_df._get_numeric_data().columns\n",
    "cat_cols = list(set(train_df.columns) - set(num_cols))\n",
    "\n",
    "print (\"There are \" + str(len(num_cols)) + \" numerical columns in the dataset (including one target column)\")\n",
    "print (\"There are \" + str(len(cat_cols)) + \" object type columns in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Dataset Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset shape\n",
    "print (f'Shape of training data :{train_df.shape}')\n",
    "print (f'Shape of testing data: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Missing Features count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(train_df.isnull().sum().sort_values(ascending=True), columns = ['Null_Counts'])\n",
    "nulls_filtered = nulls[nulls['Null_Counts'] > 0]\n",
    "if(nulls_filtered.shape[0] < 1):\n",
    "    missing_flag = False\n",
    "    print(\"There are no missing values in dataset \")\n",
    "else:\n",
    "    missing_flag = True\n",
    "    missing_cols =  nulls_filtered.index\n",
    "    numeric_missing_cols =  np.intersect1d(num_cols,missing_cols )\n",
    "    categorical_missing_cols = np.intersect1d( cat_cols ,missing_cols )\n",
    "    print (f' Total number of columns missing : { nulls_filtered.shape[0] }')\n",
    "    print (f' Number of numerical columns missing : {len(numeric_missing_cols)} ')\n",
    "    print (f' Number of objects columns missing : {len(categorical_missing_cols)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Segregate Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## segregate feature and target\n",
    "target = \"User_ID\"\n",
    "y = train_df[target]\n",
    "distinct_Y = y.value_counts().index\n",
    "\n",
    "##  we will remove id   from feature set\n",
    "\n",
    "_id = \"ID\"\n",
    "\n",
    "if _id == \"\": ## if id is not present, create a dummy \n",
    "    _id = 'id'\n",
    "    train_df[_id] = 1\n",
    "    test_df[_id] = 1\n",
    "if _id not in list(test_df.columns):\n",
    "    test_df[_id] = 1   \n",
    "\n",
    "## drop the id columns : we will remove target at end \n",
    "test_id = test_df[_id]\n",
    "train_df = train_df.drop([_id], axis=1)\n",
    "test_df = test_df.drop([_id], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[target].hist(bins = 100, figsize = (10,10),density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Correlation-Matrix  (For Numeric Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most correlated features\n",
    "# todo -> have to use get_corr values later \n",
    "\n",
    "get_corr = False\n",
    "corr = train_copy.corr()\n",
    "if len(corr) > 0:\n",
    "    get_corr = True\n",
    "    top_corr_features = corr.index[abs(corr[target])>0.4]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    g = sns.heatmap(train_copy[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "else:\n",
    "    print (\"No numeric variables for correlation plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Category Distribution (For Categorical Features)\n",
    "Lets see how many category each categorical variable has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of categories in categorical feature\n",
    "col_list = train_df.select_dtypes(include='object').columns\n",
    "cat_df = pd.DataFrame(columns= [\"Feature\",\"Category_Count\"])\n",
    "category_flag = False\n",
    "if(len(col_list)!=0):\n",
    "    category_flag= True\n",
    "    cat_df = pd.DataFrame(columns= [\"Feature\",\"Category_Count\"])\n",
    "    for i in col_list: \n",
    "        n =  train_df[i].nunique()\n",
    "        cat_df = cat_df.append({'Features': i, 'Category_Count': n }, ignore_index=True)\n",
    "        cat_df.sort_values(\"Category_Count\", axis = 0, ascending = True, inplace = True, na_position ='last')\n",
    "    size = max(10, cat_df.shape[0])\n",
    "    cat_df.plot(kind='barh',y='Category_Count', x ='Features', title = \" No of Categories\",color = ['g','y','b' ],stacked=True,legend= False,\n",
    "                figsize=(size,size/2))\n",
    "else:\n",
    "    \n",
    "    print (\"No categorical variables in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Missing Values Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (missing_flag):\n",
    "    row_count = train_df.shape[0]\n",
    "    nulls_filtered['Null_percent'] = nulls_filtered['Null_Counts']/row_count\n",
    "    nulls_filtered.plot(kind='barh',y='Null_percent',title = \"Missing features percentage\",\n",
    "                    color = ['g','y','r' ],stacked=True,legend= False)\n",
    "    print (f' Total No of missing columns: { nulls_filtered.shape[0] }')\n",
    "    print (f' No of numerical columns missing: {len(numeric_missing_cols)} ')\n",
    "    print (f' No of objects columns missing: {len(categorical_missing_cols)} ')\n",
    "else:\n",
    "    print(\"There are no missing values in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Data Prepration & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dropping Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping additional training set features (if any) wrt to test set\n",
    "if (train_df.shape[1]-test_df.shape[1] >1):\n",
    "    uncommon_feature = set(train_df.columns)- set(test_df.columns)\n",
    "    uncommon_feature.remove(target)\n",
    "    print(f'Dropping additional features: {uncommon_feature}')\n",
    "    train_df.drop((list(uncommon_feature)), axis = 1 ,inplace = True)\n",
    "else :\n",
    "    print('No additional features to drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dropping Less Correlated Numeric Features\n",
    "\n",
    "Lets Remove less co-related numerical features\n",
    "\n",
    "- threshhold_no_of_feature = 10 \n",
    "- threshhold_corr_value = 0.4\n",
    "- We will keep more than 'threshhold_no_of_feature' only if feature has correlation value larger than \n",
    "'threshhold_corr_value'\n",
    "- Note: if you feel above value are not good enough for your dataset, reset this value and rerun the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping less co-related numerical features\n",
    "if(get_corr):\n",
    "    threshhold_no_of_feature = 10 \n",
    "    threshhold_corr_value = 0.4\n",
    "    corr_matrix =  corr.abs()    \n",
    "    cm = pd.DataFrame(corr_matrix[target].sort_values(ascending=True) )\n",
    "    cm['Features'] =cm.index\n",
    "    cm = cm.reset_index()\n",
    "    least_num_cor_feature = []\n",
    "    for index, row in cm.iterrows():\n",
    "        if index > threshhold_no_of_feature and row[target] < threshhold_corr_value :\n",
    "            least_num_cor_feature.append(row[\"Features\"])\n",
    "    # drop the lesser corelated fetures from dataset \n",
    "    print (f'Shape of training data :{train_df.shape}')\n",
    "    print (f'Shape of training data :{test_df.shape}')\n",
    "    try:\n",
    "        test_df =  test_df.drop((least_num_cor_feature), axis = 1)\n",
    "        train_df = train_df.drop((least_num_cor_feature), axis = 1)\n",
    "    except:\n",
    "        pass\n",
    "    print (f'Shape of training data after less correlated Numerical features removal  :{train_df.shape}')\n",
    "    print (f'Shape of testing data after less correlated Numerical features removal :{test_df.shape}')\n",
    "else:\n",
    "    print (\"No numeric variables for correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Missing Values Treatment  \n",
    "\n",
    "Rule for missing values treatment: \n",
    "\n",
    "For categorical variables: \n",
    "    - if missing percentage < 20% --> replace by mode\n",
    "    - else -> add new category as missing\n",
    "\n",
    "For continuous variables: \n",
    "    - if missing percentage > 20% --> remove feature column\n",
    "    - else --> replace by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (missing_flag):\n",
    "    #Remove features if missing percentage > 20\n",
    "    numeric_missing_cols = np.array(list(set(numeric_missing_cols)- set(least_num_cor_feature) )) \n",
    "    \n",
    "    high_missings_cols = (nulls_filtered[nulls_filtered['Null_percent'] > 0.2]).index   \n",
    "    high_missings_numeric_cols = np.intersect1d(numeric_missing_cols,high_missings_cols )\n",
    "    high_missings_categorical_cols = np.intersect1d(categorical_missing_cols,high_missings_cols )    \n",
    "    lesser_missings_numeric_cols = np.setxor1d(numeric_missing_cols,high_missings_numeric_cols)   \n",
    "    lesser_missings_categorical_cols= np.setxor1d(categorical_missing_cols,high_missings_categorical_cols )\n",
    "    \n",
    "    print ('Droping numeric features with missing percentage > 20%...')\n",
    "    test_df =  test_df.drop((high_missings_numeric_cols), axis = 1)\n",
    "    train_df = train_df.drop((high_missings_numeric_cols), axis = 1)\n",
    "    print(\"Rest numerical columns, replacing the missing values by mean...\")\n",
    "    test_df[lesser_missings_numeric_cols] = test_df[lesser_missings_numeric_cols\n",
    "                                                   ].fillna(test_df[lesser_missings_numeric_cols].mean())\n",
    "    train_df[lesser_missings_numeric_cols] = train_df[lesser_missings_numeric_cols\n",
    "                                                     ].fillna(train_df[lesser_missings_numeric_cols].mean())\n",
    "    print ('Adding new category for categorical features with missing percentage > 20%...')\n",
    "    train_df[high_missings_categorical_cols] = train_df[high_missings_categorical_cols].fillna('MISSING')\n",
    "    test_df[high_missings_categorical_cols] =  test_df[high_missings_categorical_cols].fillna('MISSING')\n",
    "    print ('Rest missing categorical features replacing by mode...')\n",
    "    test_df[lesser_missings_categorical_cols] = test_df[lesser_missings_categorical_cols].fillna(test_df[lesser_missings_categorical_cols].mode().iloc[0])\n",
    "    train_df[lesser_missings_categorical_cols] = train_df[lesser_missings_categorical_cols].fillna(train_df[lesser_missings_categorical_cols].mode().iloc[0])\n",
    "    print (f'Shape of training data after less correlated Numerical features removal  :{train_df.shape}')\n",
    "    print (f'Shape of testing data after less correlated Numerical features removal :{test_df.shape}')\n",
    "else:\n",
    "    print(\"There are no missing values in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any more missing in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (missing_flag):\n",
    "    # We have removed  mssing data from train set if test set has any missing items we will do again for test \n",
    "    test_num_cols = test_df._get_numeric_data().columns\n",
    "    test_cat_cols = list(set(test_df.columns) - set(num_cols))\n",
    "    test_row_count = test_df.shape[0]\n",
    "    \n",
    "    test_nulls = pd.DataFrame(test_df.isnull().sum().sort_values(ascending=True), columns = ['Null_Counts'])\n",
    "    test_nulls_filtered = test_nulls[test_nulls['Null_Counts'] > 0]\n",
    "    test_missing_cols =  test_nulls_filtered.index\n",
    "    \n",
    "    test_numeric_missing_cols =  np.intersect1d(test_num_cols,test_missing_cols )\n",
    "    test_categorical_missing_cols = np.intersect1d( test_cat_cols ,test_missing_cols )\n",
    "    # numeric by mean, objects by mode  \n",
    "    test_df[test_numeric_missing_cols] = test_df[test_numeric_missing_cols].fillna(test_df[test_numeric_missing_cols].mean())\n",
    "    test_df[test_categorical_missing_cols]=test_df[test_categorical_missing_cols].fillna(test_df.mode().iloc[0])\n",
    "else:\n",
    "    print(\"Skipping .. as there are no missing value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Shape of training data after missing value treatment  :{train_df.shape}')\n",
    "print (f'Shape of testing data after missing value treatment :{test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Target Skewness Transform\n",
    "\n",
    "Transform target to minimize the skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_skew = np.absolute(train_copy[target].skew()) \n",
    "target_log_skew =np.absolute( np.log(train_copy[target]).skew())\n",
    "target_sqrt_skew =np.absolute( np.sqrt(train_copy[target]).skew())\n",
    "target_square_skew = np.absolute((train_copy[target]**2).skew())\n",
    "skew_transform = 'normal'\n",
    "\n",
    "m = min(target_skew,target_log_skew,target_sqrt_skew,target_square_skew)\n",
    "\n",
    "if m == target_log_skew :\n",
    "    y = np.log(train_copy[target])\n",
    "    skew_transform = 'log'\n",
    "elif m == target_sqrt_skew :\n",
    "    y =  np.sqrt(train_copy[target])\n",
    "    skew_transform = 'sqrt'\n",
    "                \n",
    "elif m == target_square_skew :\n",
    "    y = train_copy[target]**2\n",
    "    skew_transform = 'square'\n",
    "    \n",
    "print ( \" target Skewness feature transform is \"+skew_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4  Categorical  Flag setup for one-hot-encoding  and PCA\n",
    "\n",
    "We will prefer one-hot encoding but if any feature has many categories then we will enable PCA  before one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col_flag = False\n",
    "pca_flag = False\n",
    "if(category_flag): \n",
    "    threshhold_no_of_feature = 50 \n",
    "    max_cat_count= cat_df.iloc[-1]['Category_Count']\n",
    "    #also take care of condition when no object feature \n",
    "    if (max_cat_count > threshhold_no_of_feature):\n",
    "        pca_flag = True\n",
    "    # category column flag    \n",
    "    cat_col =  len(train_df.select_dtypes(include=object).columns)\n",
    "    if (cat_col >0):\n",
    "        cat_col_flag = True\n",
    "    else:\n",
    "        cat_col_flag = False\n",
    "else:   \n",
    "    print (\"Skipping .. as no categorical variables in the dataset\")\n",
    "\n",
    "print (f'One-hot-encdoing flag : {cat_col_flag}') \n",
    "print (f'PCA flag : {pca_flag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop([target], axis=1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df, y, test_size=0.20, random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Modelling\n",
    "\n",
    "Lets create Data models.\n",
    "First we will define few useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to evalute model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"neg_mean_squared_error\"\n",
    "def model_evaluate(model, metric=metric ):\n",
    "    measure = cross_val_score(model,train_df, y, cv=3, scoring=metric )\n",
    "    measure_mean = measure.mean()\n",
    "    print(\"3 Fold Cross-Validation \" +metric+\" measure is : \" +str(measure_mean))\n",
    "    return  measure_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (cat_col_flag & pca_flag  ):\n",
    "    base_pipe = Pipeline([\n",
    "                ('onehot',OneHotEncoder(handle_unknown='ignore')),\n",
    "                ('pca',TruncatedSVD()),\n",
    "                ('sscalar',  StandardScaler(with_mean=False)),\n",
    "              ])\n",
    "elif (cat_col_flag):\n",
    "    base_pipe = Pipeline([\n",
    "                ('onehot',OneHotEncoder(handle_unknown='ignore')),\n",
    "                ('sscalar',  StandardScaler(with_mean=False)),\n",
    "              ])\n",
    "else:\n",
    "    base_pipe = Pipeline([\n",
    "                ('sscalar',  StandardScaler(with_mean=False)),\n",
    "              ])\n",
    "\n",
    "\n",
    "print('base model is :')\n",
    "base_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for model leaderboard\n",
    "models = pd.DataFrame( columns = ['Model', 'Model_Definition', 'Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Linear Ridge Regression\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  Ridge()\n",
    "\n",
    "model1_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('Ridge', model1),\n",
    "              ])\n",
    "\n",
    "model1_pipe.fit(X_train, y_train)\n",
    "y_pred = model1_pipe.predict(X_valid)\n",
    "model1_pipe_measure = model_evaluate(model1_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {   \n",
    "        \"Ridge__alpha\" : [ 1, 10, 100 ],\n",
    "           }\n",
    "model1_pipe_rs = RandomizedSearchCV( model1_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "best_model1_pipe = model1_pipe_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model1_def = best_model1_pipe.best_estimator_\n",
    "\n",
    "model1_measure =  model_evaluate(model1_def)\n",
    "\n",
    "models = models.append({'Model': 'Ridge', 'Model_Definition': model1_def, 'Score': model1_measure},\n",
    "                       ignore_index=True)\n",
    "\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model1_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Linear Lasso Regression\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 =  Lasso()\n",
    "\n",
    "model2_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('Lasso', model2),\n",
    "              ])\n",
    "\n",
    "model2_pipe.fit(X_train, y_train)\n",
    "y_pred = model2_pipe.predict(X_valid)\n",
    "model2_pipe_measure = model_evaluate(model2_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"Lasso__alpha\" : [ 0.001, 0.01, 0.1,1, 10, 100 ],\n",
    "           }\n",
    "model2_pipe_rs = RandomizedSearchCV( model2_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "best_model2_pipe = model2_pipe_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model2_def = best_model2_pipe.best_estimator_\n",
    "\n",
    "model2_measure =  model_evaluate(model2_def)\n",
    "\n",
    "models = models.append({'Model': 'Lasso', 'Model_Definition': model2_def, 'Score': model2_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model2_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 ElasticNet\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 =  ElasticNet()\n",
    "\n",
    "model3_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('ElasticNet', model3),\n",
    "              ])\n",
    "\n",
    "model3_pipe.fit(X_train, y_train)\n",
    "y_pred = model3_pipe.predict(X_valid)\n",
    "model3_pipe_measure = model_evaluate(model3_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"ElasticNet__alpha\" : [ 0.001, 0.01, 0.1,1, 10, 100 ],\n",
    "           }\n",
    "model3_pipe_rs = RandomizedSearchCV( model3_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "best_model3_pipe = model3_pipe_rs.fit(X_train, y_train)\n",
    "\n",
    "model3_def = best_model3_pipe.best_estimator_\n",
    "\n",
    "model3_measure =  model_evaluate(model3_def)\n",
    "\n",
    "models = models.append({'Model': 'ElasticNet', 'Model_Definition': model3_def, 'Score': model3_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model3_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4  SVM Regressor\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = SVR()\n",
    "model4_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('SVR', model4),\n",
    "              ])\n",
    "\n",
    "model4_pipe.fit(X_train, y_train)\n",
    "y_pred = model4_pipe.predict(X_valid)\n",
    "model4_measure = model_evaluate(model4_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"SVR__C\": [  0.001, 0.01, 0.1, 1, 10 ],\n",
    "        \"SVR__gamma\" : [ 0.001, 0.01, 0.1, 1],\n",
    "\n",
    "           }\n",
    "\n",
    "model4_pipe_rs = RandomizedSearchCV( model4_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "best_model4_pipe = model4_pipe_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model4_def = best_model4_pipe.best_estimator_\n",
    "model4_measure =  model_evaluate(model4_def)\n",
    "\n",
    "models = models.append({'Model': 'SVR', 'Model_Definition': model4_def, 'Score': model4_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model4_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.5  KNeighborsRegressor\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model5 = KNeighborsRegressor()\n",
    "model5_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('KNeighborsRegressor', model5),\n",
    "              ])\n",
    "\n",
    "model5_pipe.fit(X_train, y_train)\n",
    "y_pred = model5_pipe.predict(X_valid)\n",
    "model5_measure = model_evaluate(model5_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"KNeighborsRegressor__n_neighbors\": [  3, 5, 10 ],\n",
    "        \"KNeighborsRegressor__algorithm\" : [ 'auto' ],\n",
    "         }\n",
    "\n",
    "model5_pipe_rs = RandomizedSearchCV( model5_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "best_model5_pipe = model5_pipe_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model5_def = best_model5_pipe.best_estimator_\n",
    "\n",
    "model5_measure =  model_evaluate(model5_def)\n",
    "\n",
    "models = models.append({'Model': 'KNeighborsRegressor', 'Model_Definition': model5_def, 'Score': model5_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model5_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.6  Base Models leaderboard\n",
    "We will take two top performing base model for our voting and stacking  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models_df = models.sort_values(by='Score', ascending=False)\n",
    "base_models_df = base_models_df.set_index('Score')\n",
    "# selecting top two base model\n",
    "base_model1 = base_models_df.iloc[0]['Model_Definition']\n",
    "base_model2 = base_models_df.iloc[1]['Model_Definition']\n",
    "\n",
    "base_models_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bagging Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 RandomForestRegressor\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = RandomForestRegressor()\n",
    "model6_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('RandomForestRegressor', model6),\n",
    "              ])\n",
    "\n",
    "model6_pipe.fit(X_train, y_train)\n",
    "y_pred = model6_pipe.predict(X_valid)\n",
    "mmodel6_pipe_measure = model_evaluate(model6_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'RandomForestRegressor__n_estimators': range(50,450,150),\n",
    "\n",
    "\n",
    "           }\n",
    "model6_pipe_rs = RandomizedSearchCV( model6_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "best_model6_pipe = model6_pipe_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model6_def = best_model6_pipe.best_estimator_\n",
    "\n",
    "model6_measure =  model_evaluate(model6_def)\n",
    "\n",
    "models = models.append({'Model': 'RandomForestRegressor', 'Model_Definition': model6_def, 'Score': model6_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model6_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 ExtraTreesRegressor\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = ExtraTreesRegressor()\n",
    "model7_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('ExtraTreesRegressor', model7),\n",
    "              ])\n",
    "\n",
    "model7_pipe.fit(X_train, y_train)\n",
    "y_pred = model7_pipe.predict(X_valid)\n",
    "model7_measure = model_evaluate(model7_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'ExtraTreesRegressor__n_estimators': range(50,126,25),\n",
    "        'ExtraTreesRegressor__max_features': ['auto', None ],\n",
    "        'ExtraTreesRegressor__min_samples_leaf': range(20,50,10),\n",
    "        'ExtraTreesRegressor__min_samples_split': range(15,30,15),\n",
    "\n",
    "           }\n",
    "\n",
    "model7_pipe_rs = RandomizedSearchCV( model7_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "best_model7_pipe = model7_pipe_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model7_def = best_model7_pipe.best_estimator_\n",
    "\n",
    "model7_measure =  model_evaluate(model7_def)\n",
    "\n",
    "models = models.append({'Model': 'ExtraTreesRegressor', 'Model_Definition': model7_def, 'Score': model7_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model7_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select top bagging model for our stacking & voting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (model6_measure > model7_measure):\n",
    "    bagging_model = model6_def\n",
    "else :\n",
    "    bagging_model = model7_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Boosting Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 XGBRegressor\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = XGBRegressor( )\n",
    "model8_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('XGBRegressor', model8),\n",
    "              ])\n",
    "\n",
    "model8_pipe.fit(X_train, y_train)\n",
    "y_pred = model8_pipe.predict(X_valid)\n",
    "model8_measure = model_evaluate(model8_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"XGBRegressor__learning_rate\": [  0.1, 0.01 ],\n",
    "        \"XGBRegressor__n_estimators\" : [ 10, 100, 600],\n",
    "        \"XGBRegressor__objective\" : [ 'reg:squarederror','reg:gamma'],\n",
    "          \"XGBRegressor__max_depth\" : [ 4, 10 , 15 ],\n",
    "\n",
    "           }\n",
    "\n",
    "model8_pipe_rs = RandomizedSearchCV( model8_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "\n",
    "best_model8_pipe = model8_pipe_rs.fit(X_train, y_train)\n",
    "model8_def = best_model8_pipe.best_estimator_\n",
    "\n",
    "model8_measure =  model_evaluate(model8_def)\n",
    "models = models.append({'Model': 'XGBRegressor', 'Model_Definition': model8_def, 'Score': model8_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model8_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 LGBMRegressor\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = LGBMRegressor( )\n",
    "model9_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('LGBMRegressor', model9),\n",
    "              ])\n",
    "\n",
    "model9_pipe.fit(X_train, y_train)\n",
    "y_pred = model9_pipe.predict(X_valid)\n",
    "model9_measure = model_evaluate(model9_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"LGBMRegressor__learning_rate\": [  0.01, 0.1  ],\n",
    "         \"LGBMRegressor__num_iterations\" : [ 100, 500 ],\n",
    "           }\n",
    "\n",
    "model9_pipe_rs = RandomizedSearchCV( model9_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "\n",
    "best_model9_pipe = model9_pipe_rs.fit(X_train, y_train)\n",
    "model9_def = best_model9_pipe.best_estimator_\n",
    "\n",
    "model9_measure =  model_evaluate(model9_def)\n",
    "\n",
    "models = models.append({'Model': 'LGBMRegressor', 'Model_Definition': model9_def, 'Score': model9_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model9_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 AdaBoostRegressor\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 =  AdaBoostRegressor()\n",
    "model10_pipe = Pipeline([\n",
    "                 ('base' , base_pipe),\n",
    "                ('AdaBoostRegressor', model10),\n",
    "              ])\n",
    "\n",
    "model10_pipe.fit(X_train, y_train)\n",
    "y_pred = model10_pipe.predict(X_valid)\n",
    "model10_measure = model_evaluate(model10_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"AdaBoostRegressor__learning_rate\": [  0.01, 0.1, 1  ],\n",
    "        \"AdaBoostRegressor__n_estimators\" : [ 100, 600, 1000],\n",
    "           }\n",
    "\n",
    "model10_pipe_rs = RandomizedSearchCV( model10_pipe, params, random_state=21, cv=3, verbose=0, n_jobs=-1, scoring=metric)\n",
    "best_model10_pipe = model10_pipe_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model10_def = best_model10_pipe.best_estimator_\n",
    "\n",
    "model10_measure =  model_evaluate(model10_def)\n",
    "\n",
    "models = models.append({'Model': 'AdaBoostRegressor', 'Model_Definition': model10_def, 'Score': model10_measure},\n",
    "                       ignore_index=True)\n",
    "print('\\n'+\"Model parameters after parameter tuning\")\n",
    "best_model10_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting top bosting model for our stacking & voting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( max(model10_measure,model9_measure,model8_measure) == model8_measure):\n",
    "    boosting_model = model8_def\n",
    "elif( max(model10_measure,model9_measure,model8_measure) == model9_measure):\n",
    "    boosting_model = model9_def\n",
    "else :\n",
    "    boosting_model = model10_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Model Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = models.sort_values(by='Score', ascending=False)\n",
    "models_df = models_df.set_index('Score')\n",
    "models_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advance Modeling\n",
    "For our Advance Modeling, we will take 4 models two from base and one each from bagging and boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Voting Regressor (Averaging)\n",
    "\n",
    "Averages the individual predictions to form a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = VotingRegressor([('base_model1',base_model1),('base_model2',base_model2), ('bagging_model', bagging_model),('boosting_model',boosting_model)] )\n",
    "model11.fit(X_train,y_train)\n",
    "y_pred = model11.predict(X_valid)\n",
    "model11_measure = model_evaluate(model11)\n",
    "# adding into our model list\n",
    "models = models.append([{ 'Model': \"Voting Regressor(Averaging)\", 'Model_Definition': model11, 'Score': model11_measure }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Voting Regressor (Weighted Averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = VotingRegressor([('base_model1',base_model1),('base_model2',base_model2), ('bagging_model', bagging_model),('boosting_model',boosting_model)], weights= [0.15,0.15,0.15,0.55] )\n",
    "model12.fit(X_train,y_train)\n",
    "y_pred = model12.predict(X_valid)\n",
    "model12_measure = model_evaluate(model12)\n",
    "# adding into our model list\n",
    "models = models.append([{ 'Model': \"Voting Regressor (Weighted Averaging)\", 'Model_Definition': model12, 'Score': model12_measure }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "model13 = StackingCVRegressor(regressors=(base_model1,bagging_model,boosting_model),\n",
    "                            meta_regressor=lasso,\n",
    "                            random_state= 2018)\n",
    "X_train_matrix = X_train.as_matrix()\n",
    "y_train_matrix = y_train.as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "model13.fit(X_train_matrix,y_train_matrix)\n",
    "train_df_matrix = train_df.as_matrix()\n",
    "mesure = cross_val_score(model13,train_df_matrix, y, cv=3, scoring= metric )\n",
    "\n",
    "model13_measure = mesure.mean()\n",
    "print(\"3 Fold Cross-Validation measure is : \" +str(model13_measure))\n",
    "\n",
    "models = models.append([{ 'Model': \"StackingModel\", 'Model_Definition': model13, 'Score': model13_measure }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Final Model Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = models.sort_values(by='Score', ascending=False)\n",
    "models_df = models_df.set_index('Score')\n",
    "models_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission File\n",
    "Preparing submission file for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform unskewness\n",
    "def consider_skewness(pred,sk_transform = skew_transform ):\n",
    "    if sk_transform == 'log':\n",
    "        return np.exp(pred)\n",
    "    elif sk_transform == 'sqrt':\n",
    "        return np.square(pred)\n",
    "    elif sk_transform == 'square':\n",
    "        return np.sqrt(pred)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model from all model\n",
    "tuned_best_model_def = models_df.iloc[0]['Model_Definition']\n",
    "# prediction\n",
    "pred = tuned_best_model_def.predict(test_df)\n",
    "# unskewness\n",
    "final_pred = consider_skewness(pred)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub[_id] = test_id\n",
    "sub[target] = final_pred\n",
    "sub.to_csv(\"baseline_solution.csv\", index=False)\n",
    "print (\"Submission File Generated, here is the snapshot: \")\n",
    "print (sub.head(10))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
